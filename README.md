# Hotel Deposit Uplift Modeling

**Causal Inference Â· Customer Behavior Analytics Â· Observational A/B**

This project uses **uplift modeling (heterogeneous treatment effect / ITE)** to estimate how **deposit policy** (`deposit_type`) affects **booking cancellations** (`is_canceled`) on a per-user basis. Rather than predicting â€œwho cancels,â€ we quantify **the causal *difference***:
[
\text{uplift}(x) ;=; P(\text{cancel}\mid \text{deposit},x);-;P(\text{cancel}\mid \text{no-deposit},x)
]

## âœ¨ Key Findings

* **Counter-intuitive pattern.** Users predicted to be **most sensitive to deposits** (high uplift) actually show **lower baseline cancel probability** (â‰ˆ **0.13**) than low-sensitivity users (â‰ˆ **0.51**).
  â†’ **Interpretation:** deposit works more like a **selection filter** than a deterrent.
* **Policy:**

  * **Require deposits** from **low-sensitivity / high-risk** users.
  * **Waive deposits** for **high-sensitivity / low-risk** users and **suppress post-booking offers** to avoid regret/overthinking-induced churn.

## ğŸ“¦ Dataset

* **File:** `./archive_8/hotel_bookings.csv`
* **Size:** 119,390 bookings, 32 columns
* **Notable fields:** `deposit_type`, `is_canceled`, `lead_time`, `customer_type`, `market_segment`, `previous_cancellations`, `booking_changes`, `total_of_special_requests`, `days_in_waiting_list`, etc.

> **Note:** The dataset is excluded via `.gitignore` (`archive_8/`, `archive_8.zip`). Place the CSV at the path above before running.

## ğŸ§ª Methodology

* **Design:** Observational causal study with **Propensity Scores** and **Inverse Probability Weighting (IPW)** to reduce selection bias.
* **Learner:** **T-Learner** with **Gradient Boosting**:

  * Train **fâ‚(x)** on treated (deposit=1) with IPW.
  * Train **fâ‚€(x)** on controls (deposit=0) with IPW.
  * Compute **uplift = fâ‚(x) âˆ’ fâ‚€(x)** for each user.
* **Features (examples):** `lead_time`, `market_segment`, `previous_cancellations`, `booking_changes`, `total_of_special_requests`, `days_in_waiting_list`, one-hot hotel/segment dummies.

**Why uplift (vs. plain classification)?**
It answers *â€œwhat changes if we add/remove the deposit?â€* instead of *â€œwho cancels?â€*, enabling **policy targeting** and **ROI-aware interventions**.

## ğŸ““ Notebook Outline

* **Cell 1 â€” Data loading & project scaffold**: shape, columns, sanity checks.
* **Cell 2 â€” NaÃ¯ve A/B**: raw difference in cancel rates.
* **Cell 3 â€” Feature prep**: one-hot, targets, treatment.
* **Cell 3.5 â€” Propensity score**: logistic regression (P(T=1\mid X)).
* **Cell 3.6 â€” IPW**: stabilized weights for treated/controls.
* **Cell 4 â€” Weighted T-Learner**: train **fâ‚**, **fâ‚€** with sample weights.
* **Cell 4.1 â€” Fit sanity**: group-specific train accuracy (focus remains on effect estimation).
* **Cell 5 â€” Per-user uplift**: compute and describe distribution; deciles.
* **Cell 6 â€” Decile analysis**: cancellation vs. uplift deciles; baseline risk check.


## ğŸ“ˆ Sanity Checks (illustrative)

* **Group fits (train)**: fâ‚ (treated) â‰ˆ **0.998**, fâ‚€ (control) â‰ˆ **0.783**.
  *Note:* these are **not** the objective; the goal is **credible effect estimation**, not max classification accuracy.

## ğŸ§  Business Playbook

* **Low-sensitivity users** (high baseline risk): require **deposit / non-refundable** or use **deposit-as-credit** mechanisms to lock commitment.
* **High-sensitivity users** (low baseline risk): **waive deposit** and **avoid post-booking promotions** to prevent cognitive dissonance and cancellations.

## ğŸ›¡ï¸ Caveats

* Observational data â†’ relies on **unconfoundedness** given features; hidden confounders may remain.
* Always A/B validate policy before full rollout (e.g., staggered or geo experiments).

## ğŸ“œ License

MIT (code). Dataset license follows its original source.

---

**Contact:** [GitHub @republic1024](https://github.com/Republic1024) Â· For academic/industry collaboration on decision intelligence & causal uplift modeling.
